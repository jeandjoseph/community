{"cells":[{"cell_type":"markdown","source":["# **Incorporating Azure Document Intelligence with Azure AI Search**\n","##### **Goals:**\n","- Use Azure Document Intelligence to structure PDF files.\n","- Store structured data in Azure Data Lake Gen2 using Python.\n","- Generate 3 JSON files for Azure AI Search's Data Source, Indexes, and Indexers.\n","- Query the Azure AI Search Indexes.\n","\n","##### **Dependencies**\n","Install the necessary libraries:\n","- `!pip install azure-ai-formrecognizer==3.3.0`\n","- `!pip install azure-storage-file-datalake`\n","- All pdf files are under file folder"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"1b48397e-c997-4565-b385-2d84985b6c39"},{"cell_type":"code","source":["!pip install azure-ai-formrecognizer==3.3.0"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":true},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e413baaa-e38b-4925-a33f-3a13887b40a8"},{"cell_type":"markdown","source":["# Access the Azure Document Intelligent\n","- Make sure your model is ready to be utilized\n","- Composed Document is highly suggested\n","- Note, make sure `pip install azure-ai-formrecognizer==3.3.0` is installed"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"3845c91e-fc28-402a-a365-ae1aab77d00c"},{"cell_type":"markdown","source":["#### Setting Variables for Azure AI Document Intelligence Access\n","\n","- Change to your local environment"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"08aa0c33-8e3e-4801-9484-1a86fdabf10f"},{"cell_type":"code","source":["# Load environment variables or key values\n","df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/AI300X/ai3002/docs/configfile/az-doc-intl-config.csv\").toPandas()\n","\n","# Assign variables for Azure Document Intelligence access\n","DOC_INTELLIGENCE_ENDPOINT = df[\"DOC_INTELLIGENCE_ENDPOINT\"][0]\n","DOC_INTELLIGENCE_KEY = df[\"DOC_INTELLIGENCE_KEY\"][0]\n","\n","# Print endpoint\n","print(DOC_INTELLIGENCE_ENDPOINT)\n","\n","# Set model ID (modifiable)\n","COMPOSE_MODEL_ID=\"TaxFormsModel\""],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"35dfd194-c6b6-4a7e-8236-706da1c069d9"},{"cell_type":"markdown","source":["##### Import Libraries for Azure Form Recognizer, File Operations, and Data Manipulation"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"8813810e-bda7-439e-b28c-93e6d2fe2e75"},{"cell_type":"code","source":["from azure.core.credentials import AzureKeyCredential\n","from azure.ai.formrecognizer import DocumentAnalysisClient\n","import os\n","import pandas as pd"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"84b3c172-f5a2-44df-8726-7c84fbf14453"},{"cell_type":"markdown","source":["##### Configure Azure Form Recognizer Service\n","- Specify the endpoint and key.\n","- Provide the Compose Model ID. No need to set the Model ID as the Compose Model determines it."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"e5ab4fcd-5500-4e90-95ea-97345a941231"},{"cell_type":"code","source":["endpoint = DOC_INTELLIGENCE_ENDPOINT\n","key = DOC_INTELLIGENCE_KEY\n","model_id = COMPOSE_MODEL_ID #MODEL_ID"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d9d30e0e-b74c-486e-b67b-f335a2645093"},{"cell_type":"markdown","source":["##### Create an instance of the DocumentAnalysisClient\n","- `endpoint`: The endpoint URL of your Azure Form Recognizer resource\n","- `credential`: The API key for your Azure Form Recognizer resource"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"35a8df8e-0cbb-44be-a4c7-64ef53e53076"},{"cell_type":"code","source":["document_analysis_client = DocumentAnalysisClient(\n","    endpoint=endpoint, credential=AzureKeyCredential(key)\n",")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"30ed9521-39cb-44ab-9e52-a838dca72735"},{"cell_type":"markdown","source":["##### Read multiple 1040 PDF form files using the Compose Model in Azure AI Document Intelligence\n","- Even if the file directory changes or the structure is different, the Compose Model will still adjust accordingly.\n","- Change to your local files path"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"4869cbd6-5a2a-40de-983d-f33566c74187"},{"cell_type":"code","source":["# Directory path 1099examples\n","dir_path = \"/lakehouse/default/Files/AI300X/ai3002/docs/trainingdata/1099examples/\"\n","\n","json_data = []\n","counter = 1  # Initialize the counter\n","\n","# Loop through all the files in the directory\n","for filename in os.listdir(dir_path):\n","    if filename.endswith(\".pdf\"):\n","        # Load file\n","        file_path = os.path.join(dir_path, filename)\n","        print(file_path)\n","        # Read the file as bytes\n","        with open(file_path, \"rb\") as f:\n","            file_bytes = f.read()\n","\n","        document_analysis_client = DocumentAnalysisClient(\n","            endpoint=endpoint, credential=AzureKeyCredential(key)\n","        )\n","\n","        # Make sure your document's type is included in the list of document types the custom model can analyze\n","        response = document_analysis_client.begin_analyze_document(model_id, file_bytes)\n","        result = response.result()\n","\n","        for idx, document in enumerate(result.documents):\n","            doc_data = {}  # Reset the dictionary for each document\n","            for name, field in document.fields.items():\n","                field_value = field.value if field.value else field.content\n","                doc_data[name] = field_value\n","\n","            # Add the filename to the dictionary\n","            doc_data['Filename'] = filename\n","\n","            # Add a unique identifier to the dictionary\n","            doc_data['ID'] = counter\n","            counter += 1  # Increment the counter\n","\n","            # Append the dictionary to the list\n","            json_data.append(doc_data)        \n","\n","# print json data\n","print(json_data)\n","\n","# print structured data\n","df = pd.DataFrame(json_data)\n","\n","# Print the DataFrame\n","display(df)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false,"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bd4d1230-61f2-4a8f-8767-d435274da907"},{"cell_type":"markdown","source":["#### Storing Azure Document Intelligence Data in Azure Data Lake Gen2\n","\n","\n","- ##### Defining Two Python Functions\n","  - `initialize_storage_account(storage_account_name, storage_account_key)`: Establishes the connection.\n","  - `save_json_to_adl(json_data, file_path)`: Saves the JSON file to Azure Data Lake Storage.\n","  - `Note`, make sure `pip install azure-storage-file-datalake` is installed"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"dd5fafda-e04c-468c-bcf6-880d88f3a042"},{"cell_type":"markdown","source":["#### Creating a Python function to establish a connection to Azure Data Lake Storage."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"59ed7673-d5da-42d3-9ba1-54507aadfeed"},{"cell_type":"code","source":["from azure.storage.filedatalake import DataLakeServiceClient\n","\n","def initialize_storage_account(storage_account_name, storage_account_key):\n","    try:  \n","        global service_client\n","\n","        service_client = DataLakeServiceClient(account_url=\"{}://{}.dfs.core.windows.net\".format(\n","            \"https\", storage_account_name), credential=storage_account_key)\n","\n","    except Exception as e:\n","        print(e)      "],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"96330dfd-aa18-4e7b-9255-94fd63da7795"},{"cell_type":"markdown","source":["#### Creating a Python Function for Storing JSON Data in Azure Data Lake Storage"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2c5ee5b7-2afd-475f-9647-fec64a94f5a6"},{"cell_type":"code","source":["from azure.storage.filedatalake import DataLakeFileClient\n","import json\n","\n","def save_json_to_adl(json_data, file_path):\n","    try:\n","        # Split the file path into container name and file name\n","        container_name, file_name = file_path.split('/', 1)\n","\n","        # Get a DataLakeFileSystemClient for the container\n","        filesystem_client = service_client.get_file_system_client(container_name)\n","\n","        # Get a DataLakeFileClient for the file\n","        file_client = filesystem_client.get_file_client(file_name)\n","\n","        # Convert the JSON data to a string\n","        json_str = json.dumps(json_data)\n","\n","        # Upload the JSON string to the file\n","        file_client.upload_data(json_str, overwrite=True)\n","        \n","        print(f'Successfully uploaded JSON data to {file_path}.')\n","\n","    except Exception as e:\n","        print(e)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3cd1a495-a677-4137-8da7-545358656d05"},{"cell_type":"markdown","source":["#### Savin `json_data` content to Azure Datalake"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6453956c-cf26-432c-bedf-39c741f1a30e"},{"cell_type":"markdown","source":["#### Setting Variables for Azure Datalake Storage Access\n","\n","- Change to your local environment"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9688bb17-f8a2-48de-9e69-8084b4c0b5d4"},{"cell_type":"code","source":["# Load environment variables or key values\n","df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/AI300X/ai3002/docs/configfile/az_dls_config.csv\").toPandas()\n","\n","# Assign variables for Azure Document Intelligence access\n","storage_account_name = df[\"storage_account_name\"][0]\n","storage_account_key = df[\"storage_account_key\"][0]\n","\n","print(storage_account_name)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b0e1b4ba-23f8-4963-8588-3ccb18996023"},{"cell_type":"code","source":["# Initialize your storage account\n","\n","initialize_storage_account(storage_account_name, storage_account_key)\n","\n","# Call the function\n","index_file_name = \"f1099msc_payer\"\n","file_path = f\"custom/az-ai-search-indexes/{index_file_name}.json\"\n","\n","save_json_to_adl(json_data, file_path)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3c5193cc-1dc3-41b3-98d5-73f2ce0eaa15"},{"cell_type":"markdown","source":["#### Azure Document Intelligence & Azure AI Search Integration\n","\n","###### Goal\n","- Need an Azure AI Search\n","- Create Indexes\n","- Create Data Source\n","- Create Indexers\n","- Note: You will need to define your indexes properties however if you are using my pdf files I already done is for your\n","- All Azure Ai Search can be found from file folder"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"621a7fc7-09ab-4a1b-a47e-63a6c5e6f824"},{"cell_type":"markdown","source":["#### Creating a Python function to connect and create objects on Azure AI Search"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"abe0ee86-9378-4690-a2b5-2d5133dbc8d0"},{"cell_type":"code","source":["import requests\n","import json\n","\n","def create_az_ai_search_objects(endpoint, api_key, objecttype, jsonfile, version):\n","    # Define the URL for creating/updating the data source\n","    url = f\"https://{endpoint}.search.windows.net/{objecttype}?api-version={version}\"\n","    print(url)\n","    # Define the request headers with the query key\n","    headers = {\n","        \"api-key\": api_key,\n","        \"Content-Type\": \"application/json\"\n","    }\n","\n","    # Load your JSON data from the file\n","    with open(jsonfile, 'r') as f:\n","        data = json.load(f)\n","\n","    # Make the POST request\n","    response = requests.post(url, headers=headers, json=data)\n","    # Check the status of the request\n","    print(response.content)\n","    if response.status_code == 200 or response.status_code == 201:\n","        print(\"Request was successful.\")\n","    else:\n","        print(f\"Request failed. Status code: {response.status_code}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0d18a4ec-cebf-4a80-9d94-e725a9229fe3"},{"cell_type":"markdown","source":["#### Setting Variables for Azure AI Search Access\n","\n","- Change to your local environment"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5dd4649e-1e1c-4dfd-a816-f21c6fda1577"},{"cell_type":"code","source":["# Load environment variables or key values\n","df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/AI300X/ai3002/docs/configfile/az_ai_search_config.csv\").toPandas()\n","\n","# Assign variables for Azure Document Intelligence access\n","az_ai_search_endpoint = df[\"az_ai_search_endpoint\"][0]\n","az_ai_search_api_key = df[\"az_ai_search_api_key\"][0]\n","az_ai_search_version = df[\"az_ai_search_version\"][0]\n","\n","print(az_ai_search_version)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d0cbe84e-d9a4-4e3a-b888-a261d38bc1e5"},{"cell_type":"code","source":["# Create the data source\n","objecttype = \"datasources\"\n","jsonfile   = \"/lakehouse/default/Files/AI300X/ai-search/data_source.json\"\n","\n","create_az_ai_search_objects(az_ai_search_endpoint, az_ai_search_api_key, objecttype, jsonfile,az_ai_search_version)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c529702e-03c9-4d1f-ab72-f5c723e5c27f"},{"cell_type":"code","source":["# create the Indexes\n","objecttype = \"Indexes\"\n","jsonfile   = \"/lakehouse/default/Files/AI300X/ai-search/Indexes.json\"\n","version    = \"2024-03-01-preview\"\n","\n","create_az_ai_search_objects(az_ai_search_endpoint, az_ai_search_api_key, objecttype, jsonfile,az_ai_search_version)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"05a69328-a49e-4839-aaa7-5e19b967c451"},{"cell_type":"code","source":["# create the Indexers\n","objecttype = \"indexers\"\n","jsonfile   = \"/lakehouse/default/Files/AI300X/ai-search/indexers.json\"\n","version    = \"2024-03-01-preview\"\n","\n","create_az_ai_search_objects(az_ai_search_endpoint, az_ai_search_api_key, objecttype, jsonfile,az_ai_search_version)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"30b8fbb3-a0e3-454d-90ff-d1b92b02e322"},{"cell_type":"markdown","source":["#### Creating a Python function to query Azure AI Search indexes"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"21703bce-5e77-4247-aa5a-9b07e479c574"},{"cell_type":"code","source":["import requests\n","import json\n","import pprint\n","\n","def query_az_ai_search (api_key,endpoint,index_name,search_query,objecttype=\"Indexes\"):\n","    # Define the headers for the API request\n","    headers = {\n","        'api-key': api_key,  # replace <your-api-key> with your actual API key\n","        'Content-Type': 'application/json'\n","    }\n","\n","    # Make the API request\n","    az_ai_search_endpoint = f\"https://{endpoint}.search.windows.net/Indexes/{index_name}/docs\"\n","\n","    response = requests.get(az_ai_search_endpoint, headers=headers, params=search_query)\n","\n","    # Parse the JSON response\n","    data = response.json()\n","\n","    # Create a pretty printer\n","    pp = pprint.PrettyPrinter(indent=4)\n","\n","    # Pretty print the data\n","    pp.pprint(data)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1674f448-4ddb-4c55-bcd2-b4cd5b141246"},{"cell_type":"code","source":["# Define the parameters for the API request\n","index_name=\"f1099-msc-payer-index\"\n","search_query = {\n","    'api-version': '2023-11-01',\n","    'search': '*'\n","}\n","\n","az_ai_search_data=[]\n","\n","az_ai_search_data = query_az_ai_search (az_ai_search_api_key,az_ai_search_endpoint,index_name,search_query)\n","\n","print(az_ai_search_data)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"86ab13c2-0e8a-4edf-ba29-3fb549b0a150"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"},"language_group":"synapse_pyspark"},"widgets":{},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{"default_lakehouse":"2bb88021-eb29-4b54-b867-41d4a5b550f1","default_lakehouse_name":"LHDI","default_lakehouse_workspace_id":"aa61b33b-f170-4b49-9389-ab7fed99afa8"}}},"nbformat":4,"nbformat_minor":5}