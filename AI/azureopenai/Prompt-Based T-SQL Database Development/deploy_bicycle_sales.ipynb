{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Auzre OPenAI Python environment variable values\n",
    "- This demo shows how Microsoft Autogen can simplify the development of large language model applications. You will first learn how to create a multi-agent conversation system manually, and then see how Autogen can automate the same process for you. This will demonstrate the intelligence and versatility of Autogen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set OpenAI configuration settings values\n",
    "azure_openai_api_type        = os.environ[\"OPENAI_API_TYPE\"]\n",
    "azure_azure_openeai_key      = os.environ[\"OPENAI_API_KEY\"]\n",
    "azure_azur_openeai_endpoint  = os.environ[\"OPENAI_API_BASE\"]\n",
    "azure_openai_api_version     = os.environ[\"OPENAI_API_VERSION\"]\n",
    "azure_openai_api_model       = os.environ[\"OPENAI_API_MODEL\"]\n",
    "\n",
    "# Temperature & Tokens\n",
    "azure_openai_api_temperature = 0\n",
    "azure_openai_api_max_tokens  = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Python function to access SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "\n",
    "def execute_sql_script(sql_script):\n",
    "    # Create a connection string\n",
    "    conn_str = (\n",
    "        r'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "        r\"SERVER={az-vm-esi-labs};\"\n",
    "        r'DATABASE=master;'\n",
    "        r'Trusted_Connection=yes;'\n",
    "    )\n",
    "    # Establish a connection with the database\n",
    "    conn = pyodbc.connect(conn_str)\n",
    "\n",
    "    # Create a cursor from the connection\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Execute the SQL script\n",
    "    cursor.execute(sql_script)\n",
    "    # Fetch the results\n",
    "    results = cursor.fetchall()\n",
    "    # Commit the transaction if no errors\n",
    "    conn.commit()\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "    # Return the results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying if SQL Server is accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('master',),\n",
       " ('tempdb',),\n",
       " ('model',),\n",
       " ('msdb',),\n",
       " ('az_synapse_link',),\n",
       " ('dp300_demo_db',),\n",
       " ('AdventureWorksLT2019',),\n",
       " ('AdventureWorks2019',),\n",
       " ('AdventureWorksDW2022',),\n",
       " ('AdventureWorks2022',),\n",
       " ('AdventureWorksLT2022',),\n",
       " ('AdventureWorksDW2020',),\n",
       " ('TailspinToys2020-US',),\n",
       " ('azureopenai',)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_script = \"SELECT name FROM sys.databases\"\n",
    "sql_server = \"az-vm-esi-labs\"\n",
    "execute_sql_script(sql_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Python function to create each T-SQL script files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tsql_file(filename, content):\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Python function to send request to Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SendRequestToAzureOpenAI(\n",
    "         azure_openai_api_type\n",
    "        ,azure_oai_endpoint\n",
    "        ,azure_openai_api_version\n",
    "        ,azure_oai_key\n",
    "        ,azure_oai_model\n",
    "        ,azure_openai_api_temperature\n",
    "        ,azure_openai_api_max_tokens\n",
    "        ,messages\n",
    "        ,filename\n",
    "    ):         \n",
    "    try:    \n",
    "        import pprint  \n",
    "        # Set OpenAI configuration settings\n",
    "        openai.api_type = azure_openai_api_type\n",
    "        openai.api_base = azure_oai_endpoint\n",
    "        openai.api_version = azure_openai_api_version\n",
    "        openai.api_key = azure_oai_key\n",
    "        # Send request to Azure OpenAI model\n",
    "        print(\"Sending request for summary to Azure OpenAI endpoint...\\n\\n\")\n",
    "        response = openai.ChatCompletion.create(\n",
    "            engine=azure_oai_model,\n",
    "            temperature = azure_openai_api_temperature,\n",
    "            max_tokens  = azure_openai_api_max_tokens,\n",
    "            messages    = messages\n",
    "        )\n",
    "        output = []\n",
    "        #output = response.choices[0].message.content + \"\\n\"\n",
    "        output = (response.choices[0].message.content).replace(\"```python\",\"\").replace(\"```\",\"\").strip()\n",
    "        create_tsql_file(filename, output)\n",
    "        #execute_sql_script(output)\n",
    "        #print(output)\n",
    "    except Exception as ex:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the name of the T-SQL script file\n",
    "This structure allows for easy access to the script names and their corresponding file names, which can be useful in automating T-SQL development tasks and CI/CD.\n",
    "- createdatabase: This script likely creates a new database. The script is contained in the file createdatabase.sql.\n",
    "- createschemastg: This script probably creates a new schema for staging. The script is in the file createschemastg.sql.\n",
    "- createschemaprd: This script likely creates a new schema for production. The script is in the file createschemaprd.sql.\n",
    "- createtemptable: This script probably creates a new temporary table. The script is in the file createtemptable.sql.\n",
    "- createprdtable: This script likely creates a new production table. The script is in the file createprdtable.sql.\n",
    "- loadstagingtable: This script likely loads data into the staging table. The script is in the file loadstagingtable.sql.\n",
    "- loadprdtable: This script likely loads data into the production table. The script is in the file loadprdtable.sql.\n",
    "- createprocedure: This script likely creates a new stored procedure. The script is in the file createprocedure.sql.\n",
    "- createview: This script likely creates a new view. The script is in the file createview.sql."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize T-SQL development scripts file names.\n",
    "keys = {\n",
    "    1: (\"createdatabase\", \"create_database.sql\"),\n",
    "    2: (\"createschemastg\", \"create_stage_schema.sql\"),\n",
    "    3: (\"createschemaprd\", \"creates_prd_chema.sql\"),\n",
    "    4: (\"createtemptable\", \"create_stage_table.sql\"),\n",
    "    5: (\"createprdtable\", \"create_prd_table.sql\"),\n",
    "    6: (\"loadstagingtable\", \"load_staging_data_table.sql\"),\n",
    "    7: (\"loadprdtable\", \"load_prd_data_table.sql\"),\n",
    "    8: (\"createprocedure\", \"create_procedure.sql\"),\n",
    "    9: (\"createview\", \"create_view.sql\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading system and all user prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'system_role': 'As a Data Engineer, your task is to write a T-SQL script to '\n",
      "                'create database objects and import a CSV file with headers '\n",
      "                'from local directory into a SQL Server Database. Follow the '\n",
      "                'instructions in order.\\n'\n",
      "                'check if each object exists then drop it before creating it.\\n'\n",
      "                'Please refrain from providing system details, instructions, '\n",
      "                'or suggestions or sql or GO command.\\n'\n",
      "                'the location of the file to be loaded is '\n",
      "                'F:\\\\Demo\\\\chatgpt\\\\files\\\\bicycle_data.csv and the table name '\n",
      "                'is stg.salestmp\\n'\n",
      "                'schema:\\n'\n",
      "                '{\\n'\n",
      "                \"    'ProductId':'INT', \\n\"\n",
      "                \"    'ProductName':'VARCHAR(50)', \\n\"\n",
      "                \"    'ProductType':'VARCHAR(30)', \\n\"\n",
      "                \"    'Color':'VARCHAR(15)', \\n\"\n",
      "                \"    'OrderQuantity':'INT', \\n\"\n",
      "                \"    'Size':'VARCHAR(15)', \\n\"\n",
      "                \"    'Category':'VARCHAR(15)', \\n\"\n",
      "                \"    'Country':'VARCHAR(30)', \\n\"\n",
      "                \"    'Date':'DATE', \\n\"\n",
      "                \"    'PurchasePrice':'DECIMAL(18,2)', \\n\"\n",
      "                \"    'SellingPrice':'DECIMAL(18,2)'\"}\n",
      "{'createdatabase': 'generate a t-sql script to create a server database named '\n",
      "                   'azureopenai under master database context.'}\n",
      "{'createschemastg': 'generate a t-sql script to create a schemas named stg.'}\n",
      "{'createschemaprd': 'generate a t-sql script to create a schemas named prd.'}\n",
      "{'createtemptable': 'generate a t-sql script to create a staging table named '\n",
      "                    'stg.salestmp with all columns as varchar(255).'}\n",
      "{'createprdtable': 'generate a t-sql script to create a table named prd.sales '\n",
      "                   'with the correct {schema} definition for each column.'}\n",
      "{'loadstagingtable': 'provide just the bulk load t-sql script without create '\n",
      "                     'table statement'}\n",
      "{'loadprdtable': 'provide just the insert t-sql script to load the data from '\n",
      "                 'stg.salestmp into prd.sales without create table statement. '\n",
      "                 'keep in mind stg.salestmp has all columns as varchar(255)'}\n",
      "{'createprocedure': 'generate a script to create a stored procedure named '\n",
      "                    'prd.usp_GetTotalSalesByCountries with an input parameter '\n",
      "                    'called country to calculate the total PurchasePrice by '\n",
      "                    'country, ProductName, and ProductType, ordered by total '\n",
      "                    'PurchasePrice in descending order. The input parameter '\n",
      "                    'country should be of type varchar(50) and default to all '\n",
      "                    'countries but the user should be able to overwrite the '\n",
      "                    'default value.'}\n",
      "{'createview': 'generate a script to create a view named prd.vw_GetSaleDetails '\n",
      "               'to return country, Color, Category.'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "json_file = 'bicycle_data_prompt.json'\n",
    "\n",
    "with open(json_file) as json_data:\n",
    "    data = json.load(json_data)\n",
    "\n",
    "filedir = \"F:\\\\Presentation\\\\Unlocking AI Potential with Prompt Engineering\\\\tsql\\\\\"\n",
    "deploymenttype = \"CreateBicycleDatbaseObjects\" #\"LoginErrorHandling\" #  \n",
    "index = 0\n",
    "while index< len(data[deploymenttype]):\n",
    "    pprint(data[deploymenttype][index])\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Python function to extract the system and all user prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GeneratePromptFilename(deploymenttype, filedir, data, index, keys):\n",
    "    system_role = data[deploymenttype][0]['system_role'] \n",
    "    # Check if the index is in the keys then return the user prompt and filename\n",
    "    if index in keys:\n",
    "        user_prompt = data[deploymenttype][index][keys[index][0]]\n",
    "        filename = filedir + keys[index][1]\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\", \"content\": system_role\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \"content\": user_prompt\n",
    "            }    \n",
    "        ]\n",
    "        return messages, filename\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending request to Azure OpenAI\n",
    "- Call GeneratePromptFilename to define the deployment type\n",
    "    - Get the file name based on the index id\n",
    "    - Get the system role prompt\n",
    "    - Get the user role prompt based on the index id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending request for summary to Azure OpenAI endpoint...\n",
      "\n",
      "\n",
      ".\\tsql\\create_database.sql\n",
      "Sending request for summary to Azure OpenAI endpoint...\n",
      "\n",
      "\n",
      ".\\tsql\\create_stage_schema.sql\n",
      "Sending request for summary to Azure OpenAI endpoint...\n",
      "\n",
      "\n",
      ".\\tsql\\creates_prd_chema.sql\n",
      "Sending request for summary to Azure OpenAI endpoint...\n",
      "\n",
      "\n",
      ".\\tsql\\create_stage_table.sql\n",
      "Sending request for summary to Azure OpenAI endpoint...\n",
      "\n",
      "\n",
      ".\\tsql\\create_prd_table.sql\n",
      "Sending request for summary to Azure OpenAI endpoint...\n",
      "\n",
      "\n",
      ".\\tsql\\load_staging_data_table.sql\n",
      "Sending request for summary to Azure OpenAI endpoint...\n",
      "\n",
      "\n",
      ".\\tsql\\load_prd_data_table.sql\n",
      "Sending request for summary to Azure OpenAI endpoint...\n",
      "\n",
      "\n",
      ".\\tsql\\create_procedure.sql\n",
      "Sending request for summary to Azure OpenAI endpoint...\n",
      "\n",
      "\n",
      ".\\tsql\\create_view.sql\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# from pprint import pprint\n",
    "\n",
    "json_file = 'bicycle_data_prompt.json'\n",
    "\n",
    "with open(json_file) as json_data:\n",
    "    data = json.load(json_data)\n",
    "\n",
    "filedir = \".\\\\tsql\\\\\"\n",
    "deploymenttype = \"CreateBicycleDatbaseObjects\" #\"LogErrorHandling\" #\n",
    "\n",
    "index = 1\n",
    "\n",
    "while index< len(data[deploymenttype]):\n",
    "    #print(index)\n",
    "    messages,filename = GeneratePromptFilename(deploymenttype,filedir,data,index,keys)\n",
    "    if __name__ == '__main__': \n",
    "        SendRequestToAzureOpenAI (\n",
    "            azure_openai_api_type\n",
    "            ,azure_azur_openeai_endpoint\n",
    "            ,azure_openai_api_version\n",
    "            ,azure_azure_openeai_key\n",
    "            ,azure_openai_api_model\n",
    "            ,azure_openai_api_temperature\n",
    "            ,azure_openai_api_max_tokens\n",
    "            ,messages\n",
    "            ,filename\n",
    "        )\n",
    "    print(filename)\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Login and Error Handling process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize T-SQL development scripts file names.\n",
    "keys = {\n",
    "    1: (\"createprocessschema\", \"create_etl_process_schema.sql\"),\n",
    "    2: (\"createprocesslogtable\", \"create_etl_processlog_table.sql\"),\n",
    "    3: (\"createbatcherrorlogtable\", \"create_etl_errorlog_table.sql\"),\n",
    "    4: (\"createprocesslogsp\", \"create_etl_processlog_usp.sql\"),\n",
    "    5: (\"createerrorlogsp\", \"create_etl_errorlog_usp.sql\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the system and user role prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'createprocessschema': 'Provide a tsql script to create the etl_process '\n",
      "                        'schema if it does not already exist.'}\n",
      "{'createprocesslogtable': 'Provide a tsql script to create the '\n",
      "                          'etl_process.etl_process table if it does not '\n",
      "                          'already exist with the following fields name id '\n",
      "                          'integer auto generated identifier without primary '\n",
      "                          'key, processname varchar 50 lenght, processtype '\n",
      "                          'varchar 30 lenght, objectname varchar 50 lenght, '\n",
      "                          'starttime and endtime: DATETIME'}\n",
      "{'createbatcherrorlogtable': 'Provide a tsql script to create the '\n",
      "                             'etl_process.error_log table if it does not '\n",
      "                             'already exist with the following fields name id '\n",
      "                             'integer auto generated identifier without '\n",
      "                             'primary key, processid integer, processname '\n",
      "                             'varchar, objectname varchar 50 lenght, errormsg '\n",
      "                             'varchar, starttime and endtime DATETIME.'}\n",
      "{'createprocesslogsp': 'Create a T-SQL stored procedure named '\n",
      "                       'etl_process.usp_get_process_log if it does not already '\n",
      "                       'exist with the following input parameters: processname '\n",
      "                       'of type VARCHAR with a length of 50, processtype of '\n",
      "                       'type VARCHAR with a length of 30, objectname of type '\n",
      "                       'VARCHAR with a length of 50, and starttime and endtime '\n",
      "                       'of type DATETIME. This stored procedure should insert '\n",
      "                       'data into an existing table called '\n",
      "                       'etl_process.etl_process_log table. Please refrain from '\n",
      "                       'providing system details, instructions, or '\n",
      "                       'suggestions.'}\n",
      "{'createerrorlogsp': 'Create a T-sQL stored procedure named '\n",
      "                     'etl_process.usp_get_error_log if it does not already '\n",
      "                     'exist with the following input parameters: processname '\n",
      "                     'of type VARCHAR with a length of 50, objectname of type '\n",
      "                     'VARCHAR with a length of 50, errormsg of type '\n",
      "                     'VARCHAR(MAX), and starttime and endtime of type '\n",
      "                     'DATETIME. This stored procedure should insert data into  '\n",
      "                     'an existing table called etl_process.error_log table.'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "json_file = 'bicycle_data_prompt.json'\n",
    "\n",
    "with open(json_file) as json_data:\n",
    "    data = json.load(json_data)\n",
    "\n",
    "deploymenttype = \"CreateLogErrorHandlingObjects\" #\"BicylOnPremPrompts\" #  \n",
    "index = 1\n",
    "while index< len(data[deploymenttype]):\n",
    "    pprint(data[deploymenttype][index])\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending request to generate Login and Error Handling Objects\n",
    "- Tables T-SQL script files\n",
    "- Store Procedures T-SQL script files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Sending request for summary to Azure OpenAI endpoint...\n",
      "\n",
      "\n",
      ".\\tsql\\create_etl_process_schema.sql\n",
      "2\n",
      "Sending request for summary to Azure OpenAI endpoint...\n",
      "\n",
      "\n",
      ".\\tsql\\create_etl_processlog_table.sql\n",
      "3\n",
      "Sending request for summary to Azure OpenAI endpoint...\n",
      "\n",
      "\n",
      ".\\tsql\\create_etl_errorlog_table.sql\n",
      "4\n",
      "Sending request for summary to Azure OpenAI endpoint...\n",
      "\n",
      "\n",
      ".\\tsql\\create_etl_processlog_usp.sql\n",
      "5\n",
      "Sending request for summary to Azure OpenAI endpoint...\n",
      "\n",
      "\n",
      ".\\tsql\\create_etl_errorlog_usp.sql\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# from pprint import pprint\n",
    "\n",
    "json_file = 'bicycle_data_prompt.json'\n",
    "\n",
    "with open(json_file) as json_data:\n",
    "    data = json.load(json_data)\n",
    "\n",
    "filedir = \".\\\\tsql\\\\\"\n",
    "deploymenttype = \"CreateLogErrorHandlingObjects\" #\"BicylOnPremPrompts\" #\n",
    "\n",
    "index = 1\n",
    "\n",
    "while index< len(data[deploymenttype]):\n",
    "    print(index)\n",
    "    messages,filename = GeneratePromptFilename(deploymenttype,filedir,data,index,keys)\n",
    "    if __name__ == '__main__': \n",
    "        SendRequestToAzureOpenAI (\n",
    "            azure_openai_api_type\n",
    "            ,azure_azur_openeai_endpoint\n",
    "            ,azure_openai_api_version\n",
    "            ,azure_azure_openeai_key\n",
    "            ,azure_openai_api_model\n",
    "            ,azure_openai_api_temperature\n",
    "            ,azure_openai_api_max_tokens\n",
    "            ,messages\n",
    "            ,filename\n",
    "        )\n",
    "    print(filename)\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Static T-SQL scripts into Dynamic T-SQL script files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BULK INSERT stg.salestmp\n",
      "FROM 'F:\\Demo\\chatgpt\\files\\bicycle_data.csv'\n",
      "WITH (\n",
      "    FIRSTROW = 2,\n",
      "    FIELDTERMINATOR = ',',\n",
      "    ROWTERMINATOR = '\\n',\n",
      "    ERRORFILE = 'F:\\Demo\\chatgpt\\files\\bicycle_data_error.log'\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create a Python function to read the T-SQL file\n",
    "def read_tsql_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        content = f.read()\n",
    "    return content\n",
    "\n",
    "# File Path\n",
    "filename= \".\\\\tsql\\\\load_staging_data_table.sql\"\n",
    "\n",
    "bulk_sql_script = read_tsql_file(filename)\n",
    "\n",
    "print(bulk_sql_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining variables value for user prompt and T-SQ: script file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert this code into a SQL Server stored procedure that performs a bulk insert from a CSV file into a table with a non default schema. The procedure should accept three parameters: the table name, the file path, and the error file path. The procedure should use try-catch blocks to handle errors and log the process and error details using two existing procedures: etl_process.usp_get_process_log with the following input parameters processname, processtype, objectname, starttime, endtime and etl_process.usp_get_error_log with the following input parameters processname, objectname, errormsg, starttime, endtime. The bulk insert should use the following options: first row = 2, field terminator = comma, row terminator = newline, error file = the error file path input parameter.\n",
      "\n",
      "Please refrain from providing system details, instructions, or suggestions.\n",
      "BULK INSERT stg.salestmp\n",
      "FROM 'F:\\Demo\\chatgpt\\files\\bicycle_data.csv'\n",
      "WITH (\n",
      "    FIRSTROW = 2,\n",
      "    FIELDTERMINATOR = ',',\n",
      "    ROWTERMINATOR = '\\n',\n",
      "    ERRORFILE = 'F:\\Demo\\chatgpt\\files\\bicycle_data_error.log'\n",
      ")\n",
      "load_staging_data_table.sql\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "json_file = 'bicycle_data_prompt.json'\n",
    "\n",
    "with open(json_file) as json_data:\n",
    "    data = json.load(json_data)\n",
    "\n",
    "filedir = \".\\\\tsql\\\\\"\n",
    "\n",
    "deploymenttype = \"MakeDatabaseObjectReusable\" #\"BicylOnPremPrompts\" # \n",
    "\n",
    "index = 1\n",
    "\n",
    "while index< len(data[deploymenttype]):\n",
    "    #pprint(data[deploymenttype][index])\n",
    "    if index == 1:\n",
    "        reusable_prompt = data[deploymenttype][index]['reusablescriptconversion']\n",
    "        user_prompt = reusable_prompt + \"\\n\" + bulk_sql_script\n",
    "        print(user_prompt)  \n",
    "    else:\n",
    "        filename = data[deploymenttype][index]['loadstagingtablefile']\n",
    "        print(filename)\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the value for the messages variable, then send a request to Azure OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending request for summary to Azure OpenAI endpoint...\n",
      "\n",
      "\n",
      ".\\tsql\\load_staging_data_table.sql\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "json_file = 'bicycle_data_prompt.json'\n",
    "\n",
    "with open(json_file) as json_data:\n",
    "    data = json.load(json_data)\n",
    "    \n",
    "filepath = \".\\\\tsql\\\\\" + filename\n",
    "\n",
    "system_role = data[deploymenttype][0]['system_role'] \n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\", \"content\": system_role\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \"content\": user_prompt\n",
    "    }    \n",
    "]\n",
    "if __name__ == '__main__': \n",
    "    SendRequestToAzureOpenAI (\n",
    "        azure_openai_api_type\n",
    "        ,azure_azur_openeai_endpoint\n",
    "        ,azure_openai_api_version\n",
    "        ,azure_azure_openeai_key\n",
    "        ,azure_openai_api_model\n",
    "        ,azure_openai_api_temperature\n",
    "        ,azure_openai_api_max_tokens\n",
    "        ,messages\n",
    "        ,filepath\n",
    "    )\n",
    "print(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To save time, letâ€™s create some objects and load the data to finish the manual process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_sql_script_file(server, database, script_file):\n",
    "    # Establish a connection to the database\n",
    "    conn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER=' + server + ';DATABASE=' + database + ';Trusted_Connection=yes;')\n",
    "\n",
    "    # Create a cursor from the connection\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Read and execute the script file line by line\n",
    "    if script_file.endswith(\".sql\"):\n",
    "        with open(script_file, \"r\") as sql_file:\n",
    "            sql_query = \"\"\n",
    "            for line in sql_file:\n",
    "                # If the line is a GO statement, execute the query and reset it\n",
    "                if line.strip() == \"GO\":\n",
    "                    cursor.execute(sql_query)\n",
    "                    sql_query = \"\"\n",
    "                # Otherwise, append the line to the query\n",
    "                else:\n",
    "                    sql_query += line\n",
    "            # Execute any remaining query\n",
    "            if sql_query:\n",
    "                cursor.execute(sql_query)\n",
    "    else:\n",
    "        sql_query = script_file\n",
    "        cursor.execute(sql_query)\n",
    "    # Commit the transaction\n",
    "    conn.commit()\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize T-SQL development scripts file names.\n",
    "keys = {\n",
    "    1: (\"createprocessschema\", \".\\\\tsql\\\\create_etl_process_schema.sql\"),\n",
    "    2: (\"createprocesslogtable\", \".\\\\tsql\\\\create_etl_processlog_table.sql\"),\n",
    "    3: (\"createbatcherrorlogtable\", \".\\\\tsql\\\\create_etl_errorlog_table.sql\"),\n",
    "    4: (\"createprocesslogsp\", \".\\\\tsql\\\\create_etl_processlog_usp.sql\"),\n",
    "    5: (\"createerrorlogsp\", \".\\\\tsql\\\\create_etl_errorlog_usp.sql\"),\n",
    "    6: (\"createstageschema\", \".\\\\tsql\\\\create_stage_schema.sql\"),\n",
    "    7: (\"createstagetable\", \".\\\\tsql\\\\create_stage_table.sql\"),\n",
    "    8: (\"loadstagingdatatable\", \".\\\\tsql\\\\load_staging_data_table.sql\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\tsql\\create_etl_process_schema.sql\n",
      ".\\tsql\\create_etl_processlog_table.sql\n",
      ".\\tsql\\create_etl_errorlog_table.sql\n",
      ".\\tsql\\create_etl_processlog_usp.sql\n",
      ".\\tsql\\create_etl_errorlog_usp.sql\n",
      ".\\tsql\\create_stage_schema.sql\n",
      ".\\tsql\\create_stage_table.sql\n",
      ".\\tsql\\load_staging_data_table.sql\n"
     ]
    }
   ],
   "source": [
    "# create schema for stage table\n",
    "script_file='.\\\\tsql\\\\create_stage_table.sql'\n",
    "database='skillupai'\n",
    "server='az-vm-esi-labs'\n",
    "index = 1\n",
    "while index<= len(keys):\n",
    "    print(keys[index][1])\n",
    "    execute_sql_script_file(sql_server, database, keys[index][1])\n",
    "    index+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store procedure to load data into stage table\n",
    "script_file=\"\"\"\n",
    "EXECUTE etl_process.usp_bulk_insert_csv \n",
    "\t\t\t 'stg.salestmp'\n",
    "\t\t\t,'F:\\\\Presentation\\\\Unlocking AI Potential with Prompt Engineering\\\\T-sql-as-Prompts\\\\csv_file\\\\bicycle_data.csv'\n",
    "\t\t\t,'F:\\\\Presentation\\\\Unlocking AI Potential with Prompt Engineering\\\\T-sql-as-Prompts\\\\csv_file\\\\errorload.csv'\n",
    "\"\"\"\n",
    "database='skillupai'\n",
    "server='az-vm-esi-labs'\n",
    "execute_sql_script_file(server, database, script_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
