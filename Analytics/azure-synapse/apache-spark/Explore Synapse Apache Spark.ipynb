{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# **Azure Synapse Spark Pool Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Query Spark configuration\r\n",
        "\r\n",
        "spark_executor_instances = spark.conf.get(\"spark.executor.instances\")\r\n",
        "print(f\"spark.executor.instances {spark_executor_instances}\")\r\n",
        "\r\n",
        "spark_executor_cores = spark.conf.get(\"spark.executor.cores\")\r\n",
        "print(f\"spark.executor.cores {spark_executor_cores}\")\r\n",
        "\r\n",
        "spark_executor_memory = spark.conf.get(\"spark.executor.memory\")\r\n",
        "print(f\"spark.executor.memory {spark_executor_memory}\")\r\n",
        "\r\n",
        "spark_driver_memory = spark.conf.get(\"spark.driver.memory\")\r\n",
        "print(f\"spark.driver.memory {spark_driver_memory}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%%configure -f\r\n",
        "{\r\n",
        "    # You can get a list of valid parameters to config the session from https://github.com/cloudera/livy#request-body.\r\n",
        "    \"driverMemory\": \"28g\", # Recommended values: [\"28g\", \"56g\", \"112g\", \"224g\", \"400g\", \"472g\"]\r\n",
        "    \"driverCores\": 4, # Recommended values: [4, 8, 16, 32, 64, 80]\r\n",
        "    \"executorMemory\": \"28g\",\r\n",
        "    \"executorCores\": 4,\r\n",
        "    \"jars\": [\"abfs[s]: //<file_system>@<account_name>.dfs.core.windows.net/<path>/myjar.jar\", \"wasb[s]: //<containername>@<accountname>.blob.core.windows.net/<path>/myjar1.jar\"],\r\n",
        "    \"conf\":\r\n",
        "    {\r\n",
        "        # Example of standard spark property, to find more available properties please visit: https://spark.apache.org/docs/latest/configuration.html#application-properties.\r\n",
        "        \"spark.driver.maxResultSize\": \"10g\",\r\n",
        "        # Example of customized property, you can specify count of lines that Spark SQL returns by configuring \"livy.rsc.sql.num-rows\".\r\n",
        "        \"livy.rsc.sql.num-rows\": \"3000\"\r\n",
        "    }\r\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Libraries supported by Azure Synapse Analytics\r\n",
        "import pkg_resources\r\n",
        "for d in pkg_resources.working_set:\r\n",
        "    print(d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 1. Generate a sample dictionary list with toy data:\r\n",
        "data = [{\"Category\": 'A', \"ID\": 1, \"Value\": 121.44, \"Truth\": True},\r\n",
        "        {\"Category\": 'B', \"ID\": 2, \"Value\": 300.01, \"Truth\": False},\r\n",
        "        {\"Category\": 'C', \"ID\": 3, \"Value\": 10.99, \"Truth\": None},\r\n",
        "        {\"Category\": 'E', \"ID\": 4, \"Value\": 33.87, \"Truth\": True}\r\n",
        "        ]\r\n",
        "\r\n",
        "# 2. Import and create a SparkSession:\r\n",
        "#from pyspark.sql import SparkSession\r\n",
        "#spark = SparkSession.builder.getOrCreate()\r\n",
        "\r\n",
        "\r\n",
        "# 3. Create a DataFrame using the createDataFrame method. Check the data type to confirm the variable is a DataFrame:\r\n",
        "df = spark.createDataFrame(data)\r\n",
        "type(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# Explore the data\r\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "pd_df = df.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "pd_df.iloc[0].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Explore Spark command\r\n",
        "# Read parquet file using spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Data Eploration & Assumptions\r\n",
        "\r\n",
        "- ## Assumptions about the contents of the data\r\n",
        "- ## Assumptions about the distribution of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      },
      "source": [
        "%%pyspark\r\n",
        "# Read Data From Azure Data Lake Storage\r\n",
        "DailyTop5Sales = spark.read.load('abfss://root@adlesilabs.dfs.core.windows.net/demofiles/csv/PurchaseOrderDetail.csv'\r\n",
        "               , format='csv'\r\n",
        "                 )\r\n",
        "\r\n",
        "display(DailyTop5Sales.limit(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Take a look to verify that, this is not the correct struct schema\r\n",
        "- timestamp vs date data type\r\n",
        "- need to do some more works to reshare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "DailyTop5Sales.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "from pyspark.sql.functions import col, to_date,sum,avg,max,count\r\n",
        "from pyspark.sql.types import DecimalType\r\n",
        "\r\n",
        "group_cols = [\"PurchaseOrderID\", \"DueDate\",\"ModifiedDate\"]\r\n",
        "\r\n",
        "DailyTop5Sales = (\r\n",
        "   DailyTop5Sales.groupBy(group_cols)\r\n",
        "          .agg(\r\n",
        "              sum(\"LineTotal\").alias(\"SubTotal\")\r\n",
        "          )\r\n",
        ").withColumn(\r\n",
        "    \"SubTotal\",col(\"SubTotal\").cast(DecimalType(18,2))\r\n",
        ").withColumn(\r\n",
        "    'DueDate', date_format(col(\"DueDate\"), \"MM-dd-yyyy\")\r\n",
        ").withColumn(\r\n",
        "    'ModifiedDate', date_format(col(\"ModifiedDate\"), \"MM-dd-yyyy\")\r\n",
        ")\r\n",
        "\r\n",
        "display(\r\n",
        "    DailyTop5Sales.select(\"PurchaseOrderID\", \"DueDate\",\"ModifiedDate\",\"SubTotal\").orderBy(col(\"SubTotal\").desc()).limit(5)\r\n",
        ")\r\n",
        "\r\n",
        "#df.orderBy(col(\"score\").desc()).head(5)\r\n",
        "#display(\r\n",
        "#    DailyTop5Sales.orderBy(col(\"SubTotal\").desc()).head(5)\r\n",
        "#)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# You can predefine the struct schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      },
      "source": [
        "%%pyspark\r\n",
        "from pyspark.sql.types import *\r\n",
        "from pyspark.sql.functions import *\r\n",
        "\r\n",
        "orderSchema = StructType([\r\n",
        "    StructField(\"PurchaseOrderID\", IntegerType()),\r\n",
        "    StructField(\"PurchaseOrderDetailID\", IntegerType()),\r\n",
        "    StructField(\"DueDate\", DateType()),\r\n",
        "    StructField(\"OrderQty\", IntegerType()),\r\n",
        "    StructField(\"ProductID\", IntegerType()),\r\n",
        "    StructField(\"UnitPrice\", DecimalType(12,2)),\r\n",
        "    StructField(\"LineTotal\", DecimalType(12,2)),\r\n",
        "    StructField(\"ReceivedQty\", DecimalType(12,2)),\r\n",
        "    StructField(\"RejectedQty\", DecimalType(12,2)),\r\n",
        "    StructField(\"StockedQty\", DecimalType(12,2)),\r\n",
        "    StructField(\"ModifiedDate\", DateType())\r\n",
        "    ])\r\n",
        "\r\n",
        "DailyTop5Sales = spark.read.load('abfss://root@adlesilabs.dfs.core.windows.net/demofiles/csv/PurchaseOrderDetail.csv'\r\n",
        "             ,format='csv'\r\n",
        "             ,schema=orderSchema\r\n",
        "             ,header=True\r\n",
        ")\r\n",
        "display(DailyTop5Sales.limit(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "DailyTop5Sales.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "group_cols = [\"PurchaseOrderID\", \"DueDate\",\"ModifiedDate\"]\r\n",
        "\r\n",
        "DailyTop5Sales = (\r\n",
        "   DailyTop5Sales.groupBy(group_cols)\r\n",
        "          .agg(\r\n",
        "              sum(\"LineTotal\").alias(\"SubTotal\")\r\n",
        "          )\r\n",
        ")\r\n",
        "\r\n",
        "#.orderBy(col(\"SubTotal\").desc())\r\n",
        "display(\r\n",
        "    DailyTop5Sales.select(\"PurchaseOrderID\", \"DueDate\",\"ModifiedDate\",\"SubTotal\").orderBy(col(\"SubTotal\").desc()).limit(5)\r\n",
        ")\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# This dataframe is Good if we do not know Python\r\n",
        "DailyTop5Sales_sql = DailyTop5Sales.select(\"PurchaseOrderID\", \"DueDate\",\"ModifiedDate\",\"SubTotal\").orderBy(col(\"SubTotal\").desc())\r\n",
        "\r\n",
        "# This dataframe is Good if we do know Python\r\n",
        "DailyTop5Sales = DailyTop5Sales.select(\"PurchaseOrderID\", \"DueDate\",\"ModifiedDate\",\"SubTotal\").orderBy(col(\"SubTotal\").desc()).limit(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Different way to select column(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "display(DailyTop5Sales[\"DueDate\", \"SubTotal\"])\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Integrate SQL and Apache Spark pools in Azure Synapse Analytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# we cant use native sql from a DataFrame\r\n",
        "# without converting it\r\n",
        "# SQL does not design to work with in memory data storage\r\n",
        "\r\n",
        "select * from DailyTop5Sales_sql"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "DailyTop5Sales_sql.createOrReplaceTempView('DailyTop5Sales')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "SELECT * FROM DailyTop5Sales ORDER BY SubTotal DESC LIMIT 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "spark.sql(\"SELECT * FROM DailyTop5Sales ORDER BY SubTotal DESC LIMIT 5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "display(spark.sql(\"SELECT * FROM DailyTop5Sales ORDER BY SubTotal DESC LIMIT 5\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "results = spark.sql(\"SELECT * FROM DailyTop5Sales ORDER BY SubTotal DESC LIMIT 5\")\r\n",
        "display(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# will fail and guest why?\r\n",
        "spark.sql(\"DROP DATABASE IF EXISTS TopDailySales CASCADE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "spark.sql(\"CREATE DATABASE IF NOT EXISTS TopDailySales\")\r\n",
        "\r\n",
        "results.write.mode(\"overwrite\").saveAsTable(\"TopDailySales.DailyTop5Sales\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "df = spark.sql(\"SELECT * FROM TopDailySales.DailyTop5Sales\")\r\n",
        "\r\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\r\n",
        "// Make sure the name of the dedcated SQL pool (SQLPool01 below) matches the name of your SQL pool.\r\n",
        "val df = spark.sqlContext.sql(\"select * from DailyTop5Sales\")\r\n",
        "\r\n",
        "df.write.synapsesql(\"SQLPool01.dbo.Top5Purchases\", Constants.INTERNAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Write using AAD Auth to internal table\r\n",
        "# Add required imports\r\n",
        "import com.microsoft.spark.sqlanalytics\r\n",
        "from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
        "\r\n",
        "# Configure and submit the request to write to Synapse Dedicated SQL Pool\r\n",
        "# Sample below is using AAD-based authentication approach; See further examples to leverage SQL Basic auth.\r\n",
        "(df.write\r\n",
        " # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
        " # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
        " .option(Constants.SERVER, \"<sql-server-name>.sql.azuresynapse.net\")\r\n",
        " # Like-wise, if `Constants.TEMP_FOLDER` is not provided, the connector will use the runtime staging directory config (see section on Configuration Options for details).\r\n",
        " .option(Constants.TEMP_FOLDER, \"abfss://<container_name>@<storage_account_name>.dfs.core.windows.net/<some_base_path_for_temporary_staging_folders>\")\r\n",
        " # Choose a save mode that is apt for your use case.\r\n",
        " # Options for save modes are \"error\" or \"errorifexists\" (default), \"overwrite\", \"append\", \"ignore\".\r\n",
        " # refer to https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes\r\n",
        " .mode(\"overwrite\")\r\n",
        " # Required parameter - Three-part table name to which data will be written\r\n",
        " .synapsesql(\"<database_name>.<schema_name>.<table_name>\"))\r\n",
        "\r\n",
        "\r\n",
        "# Write using AAD Auth to external table\r\n",
        "# Add required imports\r\n",
        "import com.microsoft.spark.sqlanalytics\r\n",
        "from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
        "\r\n",
        "# Setup and trigger the read DataFrame for write to Synapse Dedicated SQL Pool.\r\n",
        "# Sample below is using AAD-based authentication approach; See further examples to leverage SQL Basic auth.\r\n",
        "(df.write\r\n",
        " # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
        " # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
        " .option(Constants.SERVER, \"<sql-server-name>.sql.azuresynapse.net\")\r\n",
        " # Set name of the data source definition that is defined with database scoped credentials.\r\n",
        " # https://learn.microsoft.com/sql/t-sql/statements/create-external-data-source-transact-sql?view=sql-server-ver15&tabs=dedicated#h-create-external-data-source-to-access-data-in-azure-storage-using-the-abfs-interface\r\n",
        " .option(Constants.DATA_SOURCE, \"<data_source_name>\")\r\n",
        " # Choose a save mode that is apt for your use case.\r\n",
        " # Options for save modes are \"error\" or \"errorifexists\" (default), \"overwrite\", \"append\", \"ignore\".    \r\n",
        " # refer to https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes \r\n",
        " .mode(\"overwrite\")\r\n",
        " # Required parameter - Three-part table name to which data will be written\r\n",
        " .synapsesql(\"<database_name>.<schema_name>.<table_name>\",\r\n",
        "             # Optional Parameter which is used to specify table type. Default is internal i.e. Constants.INTERNAL. \r\n",
        "             # For external table type, the value is Constants.EXTERNAL.\r\n",
        "             Constants.EXTERNAL,\r\n",
        "             # Optional parameter that is used to specify external table's base folder; defaults to `database_name/schema_name/table_name`\r\n",
        "             \"/path/to/external/table\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "[Refrence:](https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/synapse-spark-sql-pool-import-export?tabs=scala%2Cpython1%2Cpython2%2Cpython3%2Cscala4%2Cscala5)"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}