{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Analyze data in a data lake with Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      },
      "source": [
        "%%pyspark\r\n",
        "\r\n",
        "# explore csv data files using synapse serverless spark pool\r\n",
        "\r\n",
        "df = spark.read.load('abfss://root@azuredatalakelabs.dfs.core.windows.net/DP500/csv/2019/2019.csv', format='csv'\r\n",
        "##, header=True\r\n",
        "##, inferschema=True\r\n",
        ")\r\n",
        "\r\n",
        "display(df.limit(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Reads data from all of the CSV files in the folder\r\n",
        "- Define schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      },
      "source": [
        "%%pyspark\r\n",
        "\r\n",
        "from pyspark.sql.types import *\r\n",
        "from pyspark.sql.functions import *\r\n",
        "\r\n",
        "orderSchema = StructType([\r\n",
        "    StructField(\"SalesOrderNumber\", StringType()),\r\n",
        "    StructField(\"SalesOrderLineNumber\", IntegerType()),\r\n",
        "    StructField(\"OrderDate\", DateType()),\r\n",
        "    StructField(\"CustomerName\", StringType()),\r\n",
        "    StructField(\"Email\", StringType()),\r\n",
        "    StructField(\"Item\", StringType()),\r\n",
        "    StructField(\"Quantity\", IntegerType()),\r\n",
        "    StructField(\"UnitPrice\", FloatType()),\r\n",
        "    StructField(\"Tax\", FloatType())\r\n",
        "    ])\r\n",
        "\r\n",
        "df = spark.read.load('abfss://root@azuredatalakelabs.dfs.core.windows.net/DP500/csv/*/*.csv', format='csv', schema=orderSchema)\r\n",
        "display(df.limit(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      },
      "source": [
        "%%pyspark\r\n",
        "\r\n",
        "# Selecting fields\r\n",
        "customers = df['CustomerName', 'Email']\r\n",
        "\r\n",
        "# display number of customers\r\n",
        "print(str(customers.count()) + \" of customers\")\r\n",
        "\r\n",
        "# display distinct number of customers\r\n",
        "print(str(customers.distinct().count()) + \" of distinct customers\")\r\n",
        "\r\n",
        "# display 5 distinct number of customers\r\n",
        "display(customers.distinct().limit(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Aggregate and group data in a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "yearlySales =\\\r\n",
        "df.select(year(\"OrderDate\")\\\r\n",
        "  .alias(\"Year\"))\\\r\n",
        "  .groupBy(\"Year\")\\\r\n",
        "  .count().orderBy(\"Year\")\r\n",
        "  \r\n",
        "display(yearlySales)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Query data using Spark SQL\r\n",
        "**createOrReplaceTempView**\r\n",
        "- Spark SQL views are lazily evaluated meaning it does not persist in memory unless you cache the dataset by using the cache() method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "df.createOrReplaceTempView(\"salesorders\")\r\n",
        "\r\n",
        "spark_df = spark.sql(\"SELECT * FROM salesorders\")\r\n",
        "\r\n",
        "display(spark_df.limit(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "SELECT YEAR(OrderDate) AS OrderYear,\r\n",
        "       SUM((UnitPrice * Quantity) + Tax) AS GrossRevenue\r\n",
        "FROM salesorders\r\n",
        "GROUP BY YEAR(OrderDate)\r\n",
        "ORDER BY OrderYear;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## View results as a built-in chart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "SELECT * FROM salesorders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## You can leverage matplotlib\r\n",
        "- Matplotlib is an amazing visualization library in Python for 2D plots of arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "sqlQuery = \"SELECT CAST(YEAR(OrderDate) AS CHAR(4)) AS OrderYear, \\\r\n",
        "                SUM((UnitPrice * Quantity) + Tax) AS GrossRevenue \\\r\n",
        "            FROM salesorders \\\r\n",
        "            GROUP BY CAST(YEAR(OrderDate) AS CHAR(4)) \\\r\n",
        "            ORDER BY OrderYear\"\r\n",
        "df_spark = spark.sql(sqlQuery)\r\n",
        "df_spark.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from matplotlib import pyplot as plt\r\n",
        "\r\n",
        "# matplotlib requires a Pandas dataframe, not a Spark one\r\n",
        "df_sales = df_spark.toPandas()\r\n",
        "\r\n",
        "# Create a bar plot of revenue by year\r\n",
        "plt.bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'])\r\n",
        "\r\n",
        "# Display the plot\r\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Clear the plot area\r\n",
        "plt.clf()\r\n",
        "\r\n",
        "# Create a figure for 2 subplots (1 row, 2 columns)\r\n",
        "fig, ax = plt.subplots(1, 2, figsize = (10,4))\r\n",
        "\r\n",
        "# Create a bar plot of revenue by year on the first axis\r\n",
        "ax[0].bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'], color='orange')\r\n",
        "ax[0].set_title('Revenue by Year')\r\n",
        "\r\n",
        "# Create a pie chart of yearly order counts on the second axis\r\n",
        "yearly_counts = df_sales['OrderYear'].value_counts()\r\n",
        "ax[1].pie(yearly_counts)\r\n",
        "ax[1].set_title('Orders per Year')\r\n",
        "ax[1].legend(yearly_counts.keys().tolist())\r\n",
        "\r\n",
        "# Add a title to the Figure\r\n",
        "fig.suptitle('Sales Data')\r\n",
        "\r\n",
        "# Show the figure\r\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Seaborn is an amazing visualization library for statistical graphics plotting in Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import seaborn as sns\r\n",
        "\r\n",
        "# Clear the plot area\r\n",
        "plt.clf()\r\n",
        "\r\n",
        "# Create a bar chart\r\n",
        "ax = sns.barplot(x=\"OrderYear\", y=\"GrossRevenue\", data=df_sales)\r\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Clear the plot area\r\n",
        "plt.clf()\r\n",
        "\r\n",
        "# Create a bar chart\r\n",
        "ax = sns.lineplot(x=\"OrderYear\", y=\"GrossRevenue\", data=df_sales)\r\n",
        "plt.show()"
      ]
    }
  ]
}